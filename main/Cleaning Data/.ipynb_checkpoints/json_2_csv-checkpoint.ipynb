{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, csv, string, datetime, re\n",
    "from dateutil.parser import parse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('DATASET.json', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def remove_dots(text):\n",
    "    return  re.compile(\n",
    "        f'[{re.escape(string.punctuation)}]'\n",
    "    ).sub(' ', text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    make   = TweetTokenizer()\n",
    "    tokens = make.tokenize( remove_dots( remove_numbers(text) ) )\n",
    "    return [w.lower() for w in tokens if not w in set(stopwords.words('english'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatando datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_datetime(value):\n",
    "    time_tuple = parsedate_tz(value)\n",
    "    timestamp  = mktime_tz(time_tuple)\n",
    "    return datetime.datetime.fromtimestamp(timestamp)\n",
    "\n",
    "times = {\n",
    "    \"data\": [],\n",
    "    \"time\": [],\n",
    "    \"lat\":  [], \n",
    "    \"long\": [] \n",
    "}\n",
    "\n",
    "for x, k in zip(data['data'].apply(parse), data['geo']):\n",
    "    time_out = str(x).split(\" \")\n",
    "    geo_out  = k.replace(\"[\",\"\").replace(\"]\",\"\").split(',')\n",
    "    times[\"data\"].append(time_out[0])\n",
    "    times[\"time\"].append(str(time_out[1]).replace(\"+00:00\",\"\"))\n",
    "    times[\"lat\"].append(geo_out[0])\n",
    "    times[\"long\"].append(geo_out[1])\n",
    "    \n",
    "#Atualizações no dataframe\n",
    "data['data'] = pd.Series(times['data'])\n",
    "data['time'] = pd.Series(times['time'])\n",
    "data['lat']  = pd.Series(times['lat'])\n",
    "data['long'] = pd.Series(times['long'])\n",
    "data.pop('geo')\n",
    "data.columns = ['date', 'id_str', 'user_name', 'text', 'time', 'latitude', 'longitude']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando filtro no campo text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, x in enumerate(data['text']):\n",
    "    data.loc[k,'text'] = str(\" \".join(remove_stopwords(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando em csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"DATASET.csv\",  index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
